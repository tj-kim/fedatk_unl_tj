{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fedatk_unl_tj\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/fedatk_unl_tj/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/ubuntu/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "\n",
    "# Import FedEM based Libraries\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from utils.util_notebooks import *\n",
    "from run_experiment import *\n",
    "from models import *\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *\n",
    "from transfer_attacks.Args import *\n",
    "from transfer_attacks.TA_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 39/80 [00:00<00:00, 183.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/40 [00:00<?, ?it/s]/home/ubuntu/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ubuntu/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 40/40 [00:17<00:00,  2.34it/s]\n",
      "/home/ubuntu/fedatk_unl_tj/aggregator.py:289: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  learner.model.load_state_dict(torch.load(chkpts_path))\n"
     ]
    }
   ],
   "source": [
    "setting, num_user = \"FedAvg_adv\", 40\n",
    "exp = \"cifar10\"\n",
    "\n",
    "try: # Skip loading if already loaded\n",
    "    aggregator\n",
    "except:\n",
    "    aggregator, clients, args_ = set_args(setting, num_user,  experiment = exp) # Indicate dataset here\n",
    "\n",
    "# Load models for FAT and FedAvg\n",
    "save_path_FAT = '/home/ubuntu/fedatk_unl_tj/weights/cifar10/231031_FAT150round/FAT/'\n",
    "save_path_FedAvg =  '/home/ubuntu/fedatk_unl_tj/weights/cifar10/230922_baseline_train/fedavg/'\n",
    "\n",
    "model_FAT = copy.deepcopy(import_model_weights(num_user, setting, save_path_FAT, aggregator, args_)[0])\n",
    "model_Fedavg = import_model_weights(num_user, setting, save_path_FedAvg, aggregator, args_)[0]\n",
    "\n",
    "# del aggregator, clients, args_\n",
    "# torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain parameters for each layer\n",
    "params_FAT = model_FAT.state_dict()\n",
    "params_FedAvg = model_Fedavg.state_dict()\n",
    "\n",
    "# Just take the values of weights and bias for the model\n",
    "desired_keys = [key for key in params_FAT.keys() if 'weight' in key or 'bias' in key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 features.0.0.weight\n",
      "1 features.0.1.weight\n",
      "2 features.0.1.bias\n",
      "3 features.0.1.running_mean\n",
      "4 features.0.1.running_var\n",
      "5 features.0.1.num_batches_tracked\n",
      "6 features.1.conv.0.0.weight\n",
      "7 features.1.conv.0.1.weight\n",
      "8 features.1.conv.0.1.bias\n",
      "9 features.1.conv.0.1.running_mean\n",
      "10 features.1.conv.0.1.running_var\n",
      "11 features.1.conv.0.1.num_batches_tracked\n",
      "12 features.1.conv.1.weight\n",
      "13 features.1.conv.2.weight\n",
      "14 features.1.conv.2.bias\n",
      "15 features.1.conv.2.running_mean\n",
      "16 features.1.conv.2.running_var\n",
      "17 features.1.conv.2.num_batches_tracked\n",
      "18 features.2.conv.0.0.weight\n",
      "19 features.2.conv.0.1.weight\n",
      "20 features.2.conv.0.1.bias\n",
      "21 features.2.conv.0.1.running_mean\n",
      "22 features.2.conv.0.1.running_var\n",
      "23 features.2.conv.0.1.num_batches_tracked\n",
      "24 features.2.conv.1.0.weight\n",
      "25 features.2.conv.1.1.weight\n",
      "26 features.2.conv.1.1.bias\n",
      "27 features.2.conv.1.1.running_mean\n",
      "28 features.2.conv.1.1.running_var\n",
      "29 features.2.conv.1.1.num_batches_tracked\n",
      "30 features.2.conv.2.weight\n",
      "31 features.2.conv.3.weight\n",
      "32 features.2.conv.3.bias\n",
      "33 features.2.conv.3.running_mean\n",
      "34 features.2.conv.3.running_var\n",
      "35 features.2.conv.3.num_batches_tracked\n",
      "36 features.3.conv.0.0.weight\n",
      "37 features.3.conv.0.1.weight\n",
      "38 features.3.conv.0.1.bias\n",
      "39 features.3.conv.0.1.running_mean\n",
      "40 features.3.conv.0.1.running_var\n",
      "41 features.3.conv.0.1.num_batches_tracked\n",
      "42 features.3.conv.1.0.weight\n",
      "43 features.3.conv.1.1.weight\n",
      "44 features.3.conv.1.1.bias\n",
      "45 features.3.conv.1.1.running_mean\n",
      "46 features.3.conv.1.1.running_var\n",
      "47 features.3.conv.1.1.num_batches_tracked\n",
      "48 features.3.conv.2.weight\n",
      "49 features.3.conv.3.weight\n",
      "50 features.3.conv.3.bias\n",
      "51 features.3.conv.3.running_mean\n",
      "52 features.3.conv.3.running_var\n",
      "53 features.3.conv.3.num_batches_tracked\n",
      "54 features.4.conv.0.0.weight\n",
      "55 features.4.conv.0.1.weight\n",
      "56 features.4.conv.0.1.bias\n",
      "57 features.4.conv.0.1.running_mean\n",
      "58 features.4.conv.0.1.running_var\n",
      "59 features.4.conv.0.1.num_batches_tracked\n",
      "60 features.4.conv.1.0.weight\n",
      "61 features.4.conv.1.1.weight\n",
      "62 features.4.conv.1.1.bias\n",
      "63 features.4.conv.1.1.running_mean\n",
      "64 features.4.conv.1.1.running_var\n",
      "65 features.4.conv.1.1.num_batches_tracked\n",
      "66 features.4.conv.2.weight\n",
      "67 features.4.conv.3.weight\n",
      "68 features.4.conv.3.bias\n",
      "69 features.4.conv.3.running_mean\n",
      "70 features.4.conv.3.running_var\n",
      "71 features.4.conv.3.num_batches_tracked\n",
      "72 features.5.conv.0.0.weight\n",
      "73 features.5.conv.0.1.weight\n",
      "74 features.5.conv.0.1.bias\n",
      "75 features.5.conv.0.1.running_mean\n",
      "76 features.5.conv.0.1.running_var\n",
      "77 features.5.conv.0.1.num_batches_tracked\n",
      "78 features.5.conv.1.0.weight\n",
      "79 features.5.conv.1.1.weight\n",
      "80 features.5.conv.1.1.bias\n",
      "81 features.5.conv.1.1.running_mean\n",
      "82 features.5.conv.1.1.running_var\n",
      "83 features.5.conv.1.1.num_batches_tracked\n",
      "84 features.5.conv.2.weight\n",
      "85 features.5.conv.3.weight\n",
      "86 features.5.conv.3.bias\n",
      "87 features.5.conv.3.running_mean\n",
      "88 features.5.conv.3.running_var\n",
      "89 features.5.conv.3.num_batches_tracked\n",
      "90 features.6.conv.0.0.weight\n",
      "91 features.6.conv.0.1.weight\n",
      "92 features.6.conv.0.1.bias\n",
      "93 features.6.conv.0.1.running_mean\n",
      "94 features.6.conv.0.1.running_var\n",
      "95 features.6.conv.0.1.num_batches_tracked\n",
      "96 features.6.conv.1.0.weight\n",
      "97 features.6.conv.1.1.weight\n",
      "98 features.6.conv.1.1.bias\n",
      "99 features.6.conv.1.1.running_mean\n",
      "100 features.6.conv.1.1.running_var\n",
      "101 features.6.conv.1.1.num_batches_tracked\n",
      "102 features.6.conv.2.weight\n",
      "103 features.6.conv.3.weight\n",
      "104 features.6.conv.3.bias\n",
      "105 features.6.conv.3.running_mean\n",
      "106 features.6.conv.3.running_var\n",
      "107 features.6.conv.3.num_batches_tracked\n",
      "108 features.7.conv.0.0.weight\n",
      "109 features.7.conv.0.1.weight\n",
      "110 features.7.conv.0.1.bias\n",
      "111 features.7.conv.0.1.running_mean\n",
      "112 features.7.conv.0.1.running_var\n",
      "113 features.7.conv.0.1.num_batches_tracked\n",
      "114 features.7.conv.1.0.weight\n",
      "115 features.7.conv.1.1.weight\n",
      "116 features.7.conv.1.1.bias\n",
      "117 features.7.conv.1.1.running_mean\n",
      "118 features.7.conv.1.1.running_var\n",
      "119 features.7.conv.1.1.num_batches_tracked\n",
      "120 features.7.conv.2.weight\n",
      "121 features.7.conv.3.weight\n",
      "122 features.7.conv.3.bias\n",
      "123 features.7.conv.3.running_mean\n",
      "124 features.7.conv.3.running_var\n",
      "125 features.7.conv.3.num_batches_tracked\n",
      "126 features.8.conv.0.0.weight\n",
      "127 features.8.conv.0.1.weight\n",
      "128 features.8.conv.0.1.bias\n",
      "129 features.8.conv.0.1.running_mean\n",
      "130 features.8.conv.0.1.running_var\n",
      "131 features.8.conv.0.1.num_batches_tracked\n",
      "132 features.8.conv.1.0.weight\n",
      "133 features.8.conv.1.1.weight\n",
      "134 features.8.conv.1.1.bias\n",
      "135 features.8.conv.1.1.running_mean\n",
      "136 features.8.conv.1.1.running_var\n",
      "137 features.8.conv.1.1.num_batches_tracked\n",
      "138 features.8.conv.2.weight\n",
      "139 features.8.conv.3.weight\n",
      "140 features.8.conv.3.bias\n",
      "141 features.8.conv.3.running_mean\n",
      "142 features.8.conv.3.running_var\n",
      "143 features.8.conv.3.num_batches_tracked\n",
      "144 features.9.conv.0.0.weight\n",
      "145 features.9.conv.0.1.weight\n",
      "146 features.9.conv.0.1.bias\n",
      "147 features.9.conv.0.1.running_mean\n",
      "148 features.9.conv.0.1.running_var\n",
      "149 features.9.conv.0.1.num_batches_tracked\n",
      "150 features.9.conv.1.0.weight\n",
      "151 features.9.conv.1.1.weight\n",
      "152 features.9.conv.1.1.bias\n",
      "153 features.9.conv.1.1.running_mean\n",
      "154 features.9.conv.1.1.running_var\n",
      "155 features.9.conv.1.1.num_batches_tracked\n",
      "156 features.9.conv.2.weight\n",
      "157 features.9.conv.3.weight\n",
      "158 features.9.conv.3.bias\n",
      "159 features.9.conv.3.running_mean\n",
      "160 features.9.conv.3.running_var\n",
      "161 features.9.conv.3.num_batches_tracked\n",
      "162 features.10.conv.0.0.weight\n",
      "163 features.10.conv.0.1.weight\n",
      "164 features.10.conv.0.1.bias\n",
      "165 features.10.conv.0.1.running_mean\n",
      "166 features.10.conv.0.1.running_var\n",
      "167 features.10.conv.0.1.num_batches_tracked\n",
      "168 features.10.conv.1.0.weight\n",
      "169 features.10.conv.1.1.weight\n",
      "170 features.10.conv.1.1.bias\n",
      "171 features.10.conv.1.1.running_mean\n",
      "172 features.10.conv.1.1.running_var\n",
      "173 features.10.conv.1.1.num_batches_tracked\n",
      "174 features.10.conv.2.weight\n",
      "175 features.10.conv.3.weight\n",
      "176 features.10.conv.3.bias\n",
      "177 features.10.conv.3.running_mean\n",
      "178 features.10.conv.3.running_var\n",
      "179 features.10.conv.3.num_batches_tracked\n",
      "180 features.11.conv.0.0.weight\n",
      "181 features.11.conv.0.1.weight\n",
      "182 features.11.conv.0.1.bias\n",
      "183 features.11.conv.0.1.running_mean\n",
      "184 features.11.conv.0.1.running_var\n",
      "185 features.11.conv.0.1.num_batches_tracked\n",
      "186 features.11.conv.1.0.weight\n",
      "187 features.11.conv.1.1.weight\n",
      "188 features.11.conv.1.1.bias\n",
      "189 features.11.conv.1.1.running_mean\n",
      "190 features.11.conv.1.1.running_var\n",
      "191 features.11.conv.1.1.num_batches_tracked\n",
      "192 features.11.conv.2.weight\n",
      "193 features.11.conv.3.weight\n",
      "194 features.11.conv.3.bias\n",
      "195 features.11.conv.3.running_mean\n",
      "196 features.11.conv.3.running_var\n",
      "197 features.11.conv.3.num_batches_tracked\n",
      "198 features.12.conv.0.0.weight\n",
      "199 features.12.conv.0.1.weight\n",
      "200 features.12.conv.0.1.bias\n",
      "201 features.12.conv.0.1.running_mean\n",
      "202 features.12.conv.0.1.running_var\n",
      "203 features.12.conv.0.1.num_batches_tracked\n",
      "204 features.12.conv.1.0.weight\n",
      "205 features.12.conv.1.1.weight\n",
      "206 features.12.conv.1.1.bias\n",
      "207 features.12.conv.1.1.running_mean\n",
      "208 features.12.conv.1.1.running_var\n",
      "209 features.12.conv.1.1.num_batches_tracked\n",
      "210 features.12.conv.2.weight\n",
      "211 features.12.conv.3.weight\n",
      "212 features.12.conv.3.bias\n",
      "213 features.12.conv.3.running_mean\n",
      "214 features.12.conv.3.running_var\n",
      "215 features.12.conv.3.num_batches_tracked\n",
      "216 features.13.conv.0.0.weight\n",
      "217 features.13.conv.0.1.weight\n",
      "218 features.13.conv.0.1.bias\n",
      "219 features.13.conv.0.1.running_mean\n",
      "220 features.13.conv.0.1.running_var\n",
      "221 features.13.conv.0.1.num_batches_tracked\n",
      "222 features.13.conv.1.0.weight\n",
      "223 features.13.conv.1.1.weight\n",
      "224 features.13.conv.1.1.bias\n",
      "225 features.13.conv.1.1.running_mean\n",
      "226 features.13.conv.1.1.running_var\n",
      "227 features.13.conv.1.1.num_batches_tracked\n",
      "228 features.13.conv.2.weight\n",
      "229 features.13.conv.3.weight\n",
      "230 features.13.conv.3.bias\n",
      "231 features.13.conv.3.running_mean\n",
      "232 features.13.conv.3.running_var\n",
      "233 features.13.conv.3.num_batches_tracked\n",
      "234 features.14.conv.0.0.weight\n",
      "235 features.14.conv.0.1.weight\n",
      "236 features.14.conv.0.1.bias\n",
      "237 features.14.conv.0.1.running_mean\n",
      "238 features.14.conv.0.1.running_var\n",
      "239 features.14.conv.0.1.num_batches_tracked\n",
      "240 features.14.conv.1.0.weight\n",
      "241 features.14.conv.1.1.weight\n",
      "242 features.14.conv.1.1.bias\n",
      "243 features.14.conv.1.1.running_mean\n",
      "244 features.14.conv.1.1.running_var\n",
      "245 features.14.conv.1.1.num_batches_tracked\n",
      "246 features.14.conv.2.weight\n",
      "247 features.14.conv.3.weight\n",
      "248 features.14.conv.3.bias\n",
      "249 features.14.conv.3.running_mean\n",
      "250 features.14.conv.3.running_var\n",
      "251 features.14.conv.3.num_batches_tracked\n",
      "252 features.15.conv.0.0.weight\n",
      "253 features.15.conv.0.1.weight\n",
      "254 features.15.conv.0.1.bias\n",
      "255 features.15.conv.0.1.running_mean\n",
      "256 features.15.conv.0.1.running_var\n",
      "257 features.15.conv.0.1.num_batches_tracked\n",
      "258 features.15.conv.1.0.weight\n",
      "259 features.15.conv.1.1.weight\n",
      "260 features.15.conv.1.1.bias\n",
      "261 features.15.conv.1.1.running_mean\n",
      "262 features.15.conv.1.1.running_var\n",
      "263 features.15.conv.1.1.num_batches_tracked\n",
      "264 features.15.conv.2.weight\n",
      "265 features.15.conv.3.weight\n",
      "266 features.15.conv.3.bias\n",
      "267 features.15.conv.3.running_mean\n",
      "268 features.15.conv.3.running_var\n",
      "269 features.15.conv.3.num_batches_tracked\n",
      "270 features.16.conv.0.0.weight\n",
      "271 features.16.conv.0.1.weight\n",
      "272 features.16.conv.0.1.bias\n",
      "273 features.16.conv.0.1.running_mean\n",
      "274 features.16.conv.0.1.running_var\n",
      "275 features.16.conv.0.1.num_batches_tracked\n",
      "276 features.16.conv.1.0.weight\n",
      "277 features.16.conv.1.1.weight\n",
      "278 features.16.conv.1.1.bias\n",
      "279 features.16.conv.1.1.running_mean\n",
      "280 features.16.conv.1.1.running_var\n",
      "281 features.16.conv.1.1.num_batches_tracked\n",
      "282 features.16.conv.2.weight\n",
      "283 features.16.conv.3.weight\n",
      "284 features.16.conv.3.bias\n",
      "285 features.16.conv.3.running_mean\n",
      "286 features.16.conv.3.running_var\n",
      "287 features.16.conv.3.num_batches_tracked\n",
      "288 features.17.conv.0.0.weight\n",
      "289 features.17.conv.0.1.weight\n",
      "290 features.17.conv.0.1.bias\n",
      "291 features.17.conv.0.1.running_mean\n",
      "292 features.17.conv.0.1.running_var\n",
      "293 features.17.conv.0.1.num_batches_tracked\n",
      "294 features.17.conv.1.0.weight\n",
      "295 features.17.conv.1.1.weight\n",
      "296 features.17.conv.1.1.bias\n",
      "297 features.17.conv.1.1.running_mean\n",
      "298 features.17.conv.1.1.running_var\n",
      "299 features.17.conv.1.1.num_batches_tracked\n",
      "300 features.17.conv.2.weight\n",
      "301 features.17.conv.3.weight\n",
      "302 features.17.conv.3.bias\n",
      "303 features.17.conv.3.running_mean\n",
      "304 features.17.conv.3.running_var\n",
      "305 features.17.conv.3.num_batches_tracked\n",
      "306 features.18.0.weight\n",
      "307 features.18.1.weight\n",
      "308 features.18.1.bias\n",
      "309 features.18.1.running_mean\n",
      "310 features.18.1.running_var\n",
      "311 features.18.1.num_batches_tracked\n",
      "312 classifier.1.weight\n",
      "313 classifier.1.bias\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "for key in params_FAT.keys():\n",
    "    print(idx, key)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating adv data set\n",
      "  Memory allocated - before adv: 1151.11 MB\n",
      "0\n",
      "  Memory allocated - inside 1 init: 1151.11 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1151.11 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1169.47 MB\n",
      "1\n",
      "  Memory allocated - inside 1 init: 1150.47 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1150.47 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1159.99 MB\n",
      "2\n",
      "  Memory allocated - inside 1 init: 1141.30 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1141.30 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1150.88 MB\n",
      "3\n",
      "  Memory allocated - inside 1 init: 1132.14 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1132.14 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1141.30 MB\n",
      "4\n",
      "  Memory allocated - inside 1 init: 1122.97 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1122.97 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1132.03 MB\n",
      "5\n",
      "  Memory allocated - inside 1 init: 1113.81 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1113.81 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1122.86 MB\n",
      "6\n",
      "  Memory allocated - inside 1 init: 1103.74 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1103.74 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1113.11 MB\n",
      "7\n",
      "  Memory allocated - inside 1 init: 1094.58 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1094.58 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1104.36 MB\n",
      "8\n",
      "  Memory allocated - inside 1 init: 1085.41 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1085.41 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1094.80 MB\n",
      "9\n",
      "  Memory allocated - inside 1 init: 1076.25 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1076.25 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1085.32 MB\n",
      "10\n",
      "  Memory allocated - inside 1 init: 1067.08 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1067.08 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1076.06 MB\n",
      "11\n",
      "  Memory allocated - inside 1 init: 1057.92 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1057.92 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1067.20 MB\n",
      "12\n",
      "  Memory allocated - inside 1 init: 1048.75 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1048.75 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1058.78 MB\n",
      "13\n",
      "  Memory allocated - inside 1 init: 1038.69 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1038.69 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1048.08 MB\n",
      "14\n",
      "  Memory allocated - inside 1 init: 1029.52 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1029.52 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1038.55 MB\n",
      "15\n",
      "  Memory allocated - inside 1 init: 1020.36 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1020.36 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1030.07 MB\n",
      "16\n",
      "  Memory allocated - inside 1 init: 1011.19 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1011.19 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1020.17 MB\n",
      "17\n",
      "  Memory allocated - inside 1 init: 1002.03 MB\n",
      "  Memory allocated - inside 2 copy data loader: 1002.03 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1011.17 MB\n",
      "18\n",
      "  Memory allocated - inside 1 init: 992.86 MB\n",
      "  Memory allocated - inside 2 copy data loader: 992.86 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 1002.38 MB\n",
      "19\n",
      "  Memory allocated - inside 1 init: 983.70 MB\n",
      "  Memory allocated - inside 2 copy data loader: 983.70 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 992.78 MB\n",
      "20\n",
      "  Memory allocated - inside 1 init: 973.63 MB\n",
      "  Memory allocated - inside 2 copy data loader: 973.63 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 983.70 MB\n",
      "21\n",
      "  Memory allocated - inside 1 init: 964.47 MB\n",
      "  Memory allocated - inside 2 copy data loader: 964.47 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 973.69 MB\n",
      "22\n",
      "  Memory allocated - inside 1 init: 955.30 MB\n",
      "  Memory allocated - inside 2 copy data loader: 955.30 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 964.39 MB\n",
      "23\n",
      "  Memory allocated - inside 1 init: 946.14 MB\n",
      "  Memory allocated - inside 2 copy data loader: 946.14 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 955.53 MB\n",
      "24\n",
      "  Memory allocated - inside 1 init: 936.97 MB\n",
      "  Memory allocated - inside 2 copy data loader: 936.97 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 946.33 MB\n",
      "25\n",
      "  Memory allocated - inside 1 init: 927.81 MB\n",
      "  Memory allocated - inside 2 copy data loader: 927.81 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 937.08 MB\n",
      "26\n",
      "  Memory allocated - inside 1 init: 918.64 MB\n",
      "  Memory allocated - inside 2 copy data loader: 918.64 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 927.69 MB\n",
      "27\n",
      "  Memory allocated - inside 1 init: 908.58 MB\n",
      "  Memory allocated - inside 2 copy data loader: 908.58 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 917.70 MB\n",
      "28\n",
      "  Memory allocated - inside 1 init: 899.41 MB\n",
      "  Memory allocated - inside 2 copy data loader: 899.41 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 908.99 MB\n",
      "29\n",
      "  Memory allocated - inside 1 init: 890.25 MB\n",
      "  Memory allocated - inside 2 copy data loader: 890.25 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 899.50 MB\n",
      "30\n",
      "  Memory allocated - inside 1 init: 881.08 MB\n",
      "  Memory allocated - inside 2 copy data loader: 881.08 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 890.06 MB\n",
      "31\n",
      "  Memory allocated - inside 1 init: 871.92 MB\n",
      "  Memory allocated - inside 2 copy data loader: 871.92 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 880.89 MB\n",
      "32\n",
      "  Memory allocated - inside 1 init: 862.75 MB\n",
      "  Memory allocated - inside 2 copy data loader: 862.75 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 872.80 MB\n",
      "33\n",
      "  Memory allocated - inside 1 init: 853.59 MB\n",
      "  Memory allocated - inside 2 copy data loader: 853.59 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 862.59 MB\n",
      "34\n",
      "  Memory allocated - inside 1 init: 843.52 MB\n",
      "  Memory allocated - inside 2 copy data loader: 843.52 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 853.44 MB\n",
      "35\n",
      "  Memory allocated - inside 1 init: 834.36 MB\n",
      "  Memory allocated - inside 2 copy data loader: 834.36 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 843.33 MB\n",
      "36\n",
      "  Memory allocated - inside 1 init: 825.19 MB\n",
      "  Memory allocated - inside 2 copy data loader: 825.19 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 834.31 MB\n",
      "37\n",
      "  Memory allocated - inside 1 init: 816.03 MB\n",
      "  Memory allocated - inside 2 copy data loader: 816.03 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 825.26 MB\n",
      "38\n",
      "  Memory allocated - inside 1 init: 806.86 MB\n",
      "  Memory allocated - inside 2 copy data loader: 806.86 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 815.84 MB\n",
      "39\n",
      "  Memory allocated - inside 1 init: 797.70 MB\n",
      "  Memory allocated - inside 2 copy data loader: 797.70 MB\n",
      "  Memory allocated - inside 4 assign to train_iterator: 806.74 MB\n"
     ]
    }
   ],
   "source": [
    "num_aru = [5, 10 ,15]\n",
    "beta_params = [0.05, 0.05, 0.05]\n",
    "weight2_list = [1 ,1, 1]\n",
    "layer_threshold = [50, 50, 50]\n",
    "num_rounds = 10\n",
    "eps = 4\n",
    "\n",
    "agg_options = [\"median_sublayers\", \"median_sublayers\", \"median_sublayers\"]\n",
    "\n",
    "result_list = []\n",
    "for itt in range(len(num_aru)):\n",
    "    result_list += [{}]\n",
    "\n",
    "# setting, num_user = \"FedAvg_adv\", 20\n",
    "save_path_FedAvg_150R = save_path_FAT\n",
    "# aggregator, clients, args_ = set_args(setting, num_user,  experiment = exp)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "agg_choice = random.choices(range(3),k=num_rounds)\n",
    "\n",
    "if True:\n",
    "    print(\"updating adv data set\")\n",
    "\n",
    "    print(f\"  Memory allocated - before adv: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\n",
    "\n",
    "    # add adv dataset update\n",
    "    Fu = np.ones(num_user) * 0.4\n",
    "    # Fu[0:10] = 1\n",
    "\n",
    "    # Setting evasion attack parameters\n",
    "    x_min = torch.min(aggregator.clients[0].adv_nn.dataloader.x_data).detach().cuda()\n",
    "    x_max = torch.max(aggregator.clients[0].adv_nn.dataloader.x_data).detach().cuda()\n",
    "    atk_params = PGD_Params()\n",
    "    atk_params.set_params(batch_size=1, iteration = 10,\n",
    "                    target = -1, x_val_min = x_min, x_val_max = x_max,\n",
    "                    step_size = 0.05, step_norm = \"inf\", eps = eps, eps_norm = 2)\n",
    "\n",
    "    # Assign proportion and attack params\n",
    "    for c in range(len(aggregator.clients)):\n",
    "        if Fu[c] > 0:\n",
    "            print (c)\n",
    "            aggregator.clients[c].set_adv_params(Fu[c], atk_params)\n",
    "            aggregator.clients[c].update_advnn()\n",
    "\n",
    "            aggregator.clients[c].assign_advdataset()\n",
    "\n",
    "            # print(f\"  Memory allocated - after adv: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\n",
    "            # aggregator.clients[c].del_advnn()\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for itt in range(len(beta_params)):\n",
    "    \n",
    "    aggregator_temp = aggregator\n",
    "\n",
    "    # Perform rounds of FAT on FedAvg model\n",
    "    num_adv = num_aru[itt]\n",
    "    weight2 = 1 / num_adv * weight2_list[itt]\n",
    "    adv_id = random.sample(range(10, num_user), num_adv)  # excluding 0-9 as Fu = 1\n",
    "    beta = beta_params[itt]\n",
    "\n",
    "    test_acc_gather = []\n",
    "    adv_acc_gather = []\n",
    "    test_acc_std = []\n",
    "    adv_acc_std = []\n",
    "    cosine_gather_layers = np.zeros([num_rounds, len(desired_keys)])\n",
    "\n",
    "    aggregator_temp.tm_rate = beta\n",
    "\n",
    "    # Test performance of aggregator on data\n",
    "    aggregator_temp.load_state(dir_path=save_path_FedAvg_150R)\n",
    "    aggregator_temp.update_clients()\n",
    "    model_FA = pull_model_from_agg(aggregator_temp)\n",
    "    model_FA.eval()\n",
    "    acc, adv_acc = get_adv_acc(aggregator_temp, model_FA, eps=eps)\n",
    "\n",
    "    print(\"Test acc: \", np.mean(acc), \"adv acc: \", np.mean(adv_acc))\n",
    "    test_acc_gather += [np.mean(acc)]\n",
    "    adv_acc_gather += [np.mean(adv_acc)]\n",
    "    test_acc_std += [np.std(acc)]\n",
    "    adv_acc_std += [np.std(adv_acc)]\n",
    "\n",
    "    del model_FA, acc, adv_acc\n",
    "\n",
    "    for i in range(num_rounds):\n",
    "        # Memory profiling before round starts\n",
    "        print(f\"  Memory allocated - pre agg: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        # Per round Freq\n",
    "        aggregator_temp.aggregation_op = agg_options[itt]\n",
    "\n",
    "        # Perform UNL aggregation\n",
    "        UNL_mix(aggregator_temp, adv_id, model_inject=model_Fedavg, keys=desired_keys, \n",
    "                weight_scale_2=weight2, dump_flag=False, tm_beta=beta, \n",
    "                median_threshold=layer_threshold[itt])\n",
    "        print(f\"  Memory allocated - post UNL MIX: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\n",
    "\n",
    "        model_overfit = pull_model_from_agg(aggregator_temp)\n",
    "        model_overfit.eval()\n",
    "        acc, adv_acc = get_adv_acc(aggregator_temp, model_overfit, eps=eps)\n",
    "\n",
    "        print(f\"  Memory allocated - post ADV ACC: {torch.cuda.memory_allocated() / 1e6:.2f} MB\")\n",
    "\n",
    "        print(\"Round\", i, \"Test acc: \", np.mean(acc), \"adv acc: \", np.mean(adv_acc))\n",
    "        test_acc_gather += [np.mean(acc)]\n",
    "        adv_acc_gather += [np.mean(adv_acc)]\n",
    "        test_acc_std += [np.std(acc)]\n",
    "        adv_acc_std += [np.std(adv_acc)]\n",
    "\n",
    "        result_list[itt]['test_acc'] = test_acc_gather\n",
    "        result_list[itt]['adv_acc'] = adv_acc_gather\n",
    "        result_list[itt]['test_std'] = test_acc_std\n",
    "        result_list[itt]['adv_std'] = adv_acc_std\n",
    "        result_list[itt]['num_clients'] = num_adv\n",
    "        result_list[itt]['beta'] = beta\n",
    "        result_list[itt]['weight2'] = weight2_list[itt]\n",
    "\n",
    "        # Clean up after each round\n",
    "        del model_overfit\n",
    "    torch.cuda.empty_cache()\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    initial_lr: 0.01\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0005\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aggregator.clients[0].learners_ensemble.learners[0].optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEM_env",
   "language": "python",
   "name": "fedem_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
