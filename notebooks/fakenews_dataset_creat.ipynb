{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/sbert/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(data_dir):\n",
    "    # List to store the text and labels\n",
    "    dataset = []\n",
    "\n",
    "    # Walk through the directory structure\n",
    "    for source_folder in os.listdir(data_dir):\n",
    "        source_folder_path = os.path.join(data_dir, source_folder)\n",
    "        \n",
    "        # We expect only two subfolders: 'real' and 'fake'\n",
    "        for label_folder in ['real', 'fake']:\n",
    "            label_folder_path = os.path.join(source_folder_path, label_folder)\n",
    "            \n",
    "            # Skip if the directory doesn't exist (it could happen in case of missing data)\n",
    "            if not os.path.exists(label_folder_path):\n",
    "                continue\n",
    "\n",
    "            # Set label based on the folder (1 for real, 0 for fake)\n",
    "            label = 1 if label_folder == 'real' else 0\n",
    "            \n",
    "            # Loop through the 'some_folder' subfolders\n",
    "            for some_folder in os.listdir(label_folder_path):\n",
    "                some_folder_path = os.path.join(label_folder_path, some_folder)\n",
    "\n",
    "                # Skip if it's not a directory\n",
    "                if not os.path.isdir(some_folder_path):\n",
    "                    continue\n",
    "\n",
    "                # Now loop through the JSON files in the 'some_folder'\n",
    "                for json_file_name in os.listdir(some_folder_path):\n",
    "                    if json_file_name.endswith('.json'):\n",
    "                        json_file_path = os.path.join(some_folder_path, json_file_name)\n",
    "                        \n",
    "                        # Read the JSON file\n",
    "                        with open(json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "                            data = json.load(json_file)\n",
    "                            \n",
    "                            # Extract the 'text' field\n",
    "                            text = data.get('text', '')\n",
    "                            \n",
    "                            # Append to the dataset (text and label)\n",
    "                            dataset.append({'text': text, 'label': label, 'source': source_folder})\n",
    "\n",
    "    # Convert the list of dicts into a Pandas DataFrame\n",
    "    df = pd.DataFrame(dataset)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the root folder of the dataset\n",
    "data_dir = \"/home/ubuntu/FakeNewsNet/code/fakenewsnet_dataset\"\n",
    "df = build_dataset(data_dir)\n",
    "\n",
    "# Generate embeddings for the 'text' column\n",
    "embeddings = model.encode(df['text'].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# Add the embeddings to the dataframe as a new column\n",
    "df['embedding'] = embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    11433\n",
       "0     5239\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    3480\n",
      "1    3480\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def word_count(text):\n",
    "    return len(text.split())\n",
    "\n",
    "# Filter rows where the word count is 30 or more\n",
    "df_filtered = df[df['text'].apply(word_count) >= 100]\n",
    "df_filtered2 = df_filtered[df_filtered['text'].apply(word_count) <= 700]\n",
    "\n",
    "\n",
    "\n",
    "# Separate the dataframe by label\n",
    "df_label_1 = df_filtered2[df_filtered2['label'] == 1]\n",
    "df_label_0 = df_filtered2[df_filtered2['label'] == 0]\n",
    "\n",
    "# Get the count of the minority class (label = 0)\n",
    "minority_count = len(df_label_0)\n",
    "\n",
    "# Downsample the majority class (label = 1) to match the minority count\n",
    "df_label_1_downsampled = df_label_1.sample(n=minority_count, random_state=123)\n",
    "\n",
    "# Combine the downsampled majority class with the minority class\n",
    "df_balanced = pd.concat([df_label_1_downsampled, df_label_0])\n",
    "\n",
    "# Shuffle the resulting dataframe\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=123).reset_index(drop=True)\n",
    "\n",
    "# Check the label distribution in the new dataframe\n",
    "print(df_balanced['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_path = '/home/ubuntu/fedatk_unl_tj/data/fakenewsnet/raw_data/data.json'\n",
    "# df_filtered2.to_json(pkl_path, orient='records', lines=True)\n",
    "df_dict = df_balanced.to_dict(orient='records')\n",
    "\n",
    "with open(pkl_path, 'w') as json_file:\n",
    "    json.dump(df_dict, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sbert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
