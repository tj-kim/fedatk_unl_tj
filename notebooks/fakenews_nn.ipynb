{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fedatk_unl_tj\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/ubuntu/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Run Experiment\n",
    "\n",
    "This script allows to run one federated learning experiment; the experiment name, the method and the\n",
    "number of clients/tasks should be precised along side with the hyper-parameters of the experiment.\n",
    "\n",
    "The results of the experiment (i.e., training logs) are written to ./logs/ folder.\n",
    "\n",
    "This file can also be imported as a module and contains the following function:\n",
    "\n",
    "    * run_experiment - runs one experiments given its arguments\n",
    "\"\"\"\n",
    "\n",
    "# Import General Libraries\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import copy\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numba \n",
    "\n",
    "import sys\n",
    "\n",
    "from utils.utils import *\n",
    "from utils.constants import *\n",
    "from utils.args import *\n",
    "from run_experiment import * \n",
    "from models import *\n",
    "\n",
    "\n",
    "# Import Transfer Attack\n",
    "from transfer_attacks.Personalized_NN import *\n",
    "from transfer_attacks.Params import *\n",
    "from transfer_attacks.Transferer import *\n",
    "from transfer_attacks.Args import *\n",
    "from transfer_attacks.TA_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running trial: 0\n",
      "==> Clients initialization..\n",
      "===> Building data iterators..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [00:01<00:00, 16.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Initializing clients..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:00<00:00, 21.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 0.693 | Train Acc: 50.378% |Test Loss: 0.693 | Test Acc: 48.500% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 0.693 | Train Acc: 50.378% |Test Loss: 0.693 | Test Acc: 48.500% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "################################################################################\n",
      "Training..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/ubuntu/fedatk_unl_tj/learners/learner.py:190: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3683.)\n",
      "  loss = (loss_vec.T @ weights[indices]) / loss_vec.size(0)\n",
      " 20%|██        | 10/50 [00:04<00:19,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 0.690 | Train Acc: 50.378% |Test Loss: 0.691 | Test Acc: 48.500% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "++++++++++++++++++++++++++++++\n",
      "Global..\n",
      "Train Loss: 0.690 | Train Acc: 50.378% |Test Loss: 0.691 | Test Acc: 48.500% |\n",
      "++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "################################################################################\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8200/3932428827.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    114\u001b[0m                     \u001b[0maggregator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclients\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_advdataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0maggregator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# Save more often the intermediate NN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fedatk_unl_tj/aggregator.py\u001b[0m in \u001b[0;36mmix\u001b[0;34m(self, replace, dump_flag)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampled_clients\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 558\u001b[0;31m             \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;31m# self.record_client_updates()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fedatk_unl_tj/client.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, single_batch_flag, *args, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mclient_updates\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 self.learners_ensemble.fit_epochs(\n\u001b[0m\u001b[1;32m    182\u001b[0m                     \u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                     \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fedatk_unl_tj/learners/learners_ensemble.py\u001b[0m in \u001b[0;36mfit_epochs\u001b[0;34m(self, iterator, n_epochs, weights)\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mold_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_param_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m                 \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlearner_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fedatk_unl_tj/learners/learner.py\u001b[0m in \u001b[0;36mfit_epochs\u001b[0;34m(self, iterator, n_epochs, weights)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \"\"\"\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/fedatk_unl_tj/learners/learner.py\u001b[0m in \u001b[0;36mfit_epoch\u001b[0;34m(self, iterator, weights)\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;31m# loss.backward(retain_graph=True) # delete later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "# os.chdir(parent_dir) # As we are in a folder\n",
    "\n",
    "dataset = \"fakenewsnet\"\n",
    "exp_names =['FAT_50R']#  ['FedAvg']   \n",
    "exp_method =   ['FedAvg_adv'] # ['FedAvg']\n",
    "save_folder = 'weights/fakenews/250119_small_architecture_moreconv/'\n",
    "\n",
    "exp_num_learners = 1\n",
    "exp_lr = 0.01\n",
    "num_rounds = 50\n",
    "num_clients = 20\n",
    "FAT_start_round = 10\n",
    "    \n",
    "for itt in range(len(exp_names)):\n",
    "    \n",
    "    print(\"running trial:\", itt)\n",
    "    \n",
    "    # Manually set argument parameters\n",
    "    args_ = Args()\n",
    "    args_.experiment = dataset\n",
    "    args_.method = exp_method[itt]\n",
    "    args_.decentralized = False\n",
    "    args_.sampling_rate = 1.0\n",
    "    args_.input_dimension = None\n",
    "    args_.output_dimension = None\n",
    "    args_.n_learners= exp_num_learners\n",
    "    args_.n_rounds = num_rounds\n",
    "    args_.bz = 128\n",
    "    args_.local_steps = 1\n",
    "    args_.lr_lambda = 0\n",
    "    args_.lr = exp_lr\n",
    "    args_.lr_scheduler = 'multi_step'\n",
    "    args_.log_freq = 10\n",
    "    args_.device = 'cuda'\n",
    "    args_.optimizer = 'sgd'\n",
    "    args_.mu = 0\n",
    "    args_.communication_probability = 0.1\n",
    "    args_.q = 1\n",
    "    args_.locally_tune_clients = False\n",
    "    args_.seed = 1234\n",
    "    args_.verbose = 1\n",
    "    args_.save_path = save_folder + exp_names[itt]\n",
    "    args_.validation = False\n",
    "    args_.save_freq = 10\n",
    "\n",
    "    # Other Argument Parameters\n",
    "    Q = 10 # update per round\n",
    "    G = 0.5\n",
    "    S = 0.05 # Threshold\n",
    "    step_size = 0.01\n",
    "    K = 10\n",
    "    eps = 1.5\n",
    "\n",
    "    # Randomized Parameters\n",
    "    # Ru = np.random.uniform(0, 0.5, size=num_clients)\n",
    "    Ru = np.ones(num_clients)\n",
    "    \n",
    "    # Generate the dummy values here\n",
    "    aggregator, clients = dummy_aggregator(args_, num_clients)\n",
    "\n",
    "    # Change client datset\n",
    "    for i in range(len(clients)):\n",
    "        aggregator.clients[i].dataset_name = dataset\n",
    "\n",
    "    # Set attack parameters\n",
    "    if exp_method[itt] == 'FedAvg_adv':\n",
    "        x_min = torch.min(clients[0].adv_nn.dataloader.x_data)\n",
    "        x_max = torch.max(clients[0].adv_nn.dataloader.x_data)\n",
    "        atk_params = PGD_Params()\n",
    "        atk_params.set_params(batch_size=1, iteration = K,\n",
    "                        target = -1, x_val_min = x_min, x_val_max = x_max,\n",
    "                        step_size = 0.05, step_norm = \"inf\", eps = eps, eps_norm = 2)\n",
    "\n",
    "    # Obtain the central controller decision making variables (static)\n",
    "    num_h = args_.n_learners= 3\n",
    "    Du = np.zeros(len(clients))\n",
    "\n",
    "    for i in range(len(clients)):\n",
    "        num_data = clients[i].train_iterator.dataset.targets.shape[0]\n",
    "        Du[i] = num_data\n",
    "    D = np.sum(Du) # Total number of data points\n",
    "\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Training..\")\n",
    "    pbar = tqdm(total=args_.n_rounds)\n",
    "    current_round = 0\n",
    "    while current_round <= args_.n_rounds:\n",
    "\n",
    "        if exp_method[itt] == 'FedAvg_adv':\n",
    "            # If statement catching every Q rounds -- update dataset\n",
    "            if  current_round != 0 and current_round%Q == 0 and current_round >= FAT_start_round: # \n",
    "                # print(\"Round:\", current_round, \"Calculation Adv\")\n",
    "                # Obtaining hypothesis information\n",
    "                Whu = np.zeros([num_clients,num_h]) # Hypothesis weight for each user\n",
    "                for i in range(len(clients)):\n",
    "                    # print(\"client\", i)\n",
    "                    temp_client = aggregator.clients[i]\n",
    "                    hyp_weights = temp_client.learners_ensemble.learners_weights\n",
    "                    Whu[i] = hyp_weights\n",
    "\n",
    "                row_sums = Whu.sum(axis=1)\n",
    "                Whu = Whu / row_sums[:, np.newaxis]\n",
    "                Wh = np.sum(Whu,axis=0)/num_clients\n",
    "\n",
    "                # Solve for adversarial ratio at every client\n",
    "                # Fu = solve_proportions(G, num_clients, num_h, Du, Whu, S, Ru, step_size)\n",
    "                Fu = np.ones(num_clients) * G\n",
    "\n",
    "                # Assign proportion and attack params\n",
    "                for i in range(len(clients)):\n",
    "                    aggregator.clients[i].set_adv_params(Fu[i], atk_params)\n",
    "                    aggregator.clients[i].update_advnn()\n",
    "                    aggregator.clients[i].assign_advdataset()\n",
    "\n",
    "        aggregator.mix()\n",
    "        \n",
    "        # Save more often the intermediate NN\n",
    "        if current_round% args_.save_freq == 0:\n",
    "            if \"save_path\" in args_:\n",
    "                save_root = os.path.join(args_.save_path)\n",
    "\n",
    "                os.makedirs(save_root, exist_ok=True)\n",
    "#                     aggregator.save_state_intermed(save_root, current_round)\n",
    "\n",
    "        if aggregator.c_round != current_round:\n",
    "            pbar.update(1)\n",
    "            current_round = aggregator.c_round\n",
    "\n",
    "    if \"save_path\" in args_:\n",
    "        save_root = os.path.join(args_.save_path)\n",
    "\n",
    "        os.makedirs(save_root, exist_ok=True)\n",
    "        aggregator.save_state(save_root)\n",
    "        \n",
    "    # del args_, aggregator, clients\n",
    "    # torch.cuda.empty_cache()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
