{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/fedatk_unl_tj\n"
     ]
    }
   ],
   "source": [
    "cd /home/ubuntu/fedatk_unl_tj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "def load_indices(task_x_path):\n",
    "    \"\"\"\n",
    "    Load the indices from the task_x pickle files.\n",
    "    \"\"\"\n",
    "    # Load train indices\n",
    "    train_file = os.path.join(task_x_path, 'train.pkl')\n",
    "    with open(train_file, 'rb') as f:\n",
    "        train_indices = pickle.load(f)\n",
    "\n",
    "    # Load test indices\n",
    "    test_file = os.path.join(task_x_path, 'test.pkl')\n",
    "    with open(test_file, 'rb') as f:\n",
    "        test_indices = pickle.load(f)\n",
    "\n",
    "    return train_indices, test_indices\n",
    "\n",
    "def create_placeholder_image(image_path):\n",
    "    \"\"\"\n",
    "    Replace the image at image_path with a 1x1 black placeholder image.\n",
    "    \"\"\"\n",
    "    placeholder = Image.new(\"RGB\", (1, 1), color=(0, 0, 0))  # Black 1x1 image\n",
    "    placeholder.save(image_path, format=\"JPEG\")  # Save as JPG (format is 'JPEG' even for .jpg extension)\n",
    "\n",
    "def generate_indices_to_keep(task_x_path, client_dirs, test_offset = 182637):\n",
    "    \"\"\"\n",
    "    Generate the list of indices to keep based on the train/test indices.\n",
    "    \"\"\"\n",
    "    # Initialize list for desired indices to keep\n",
    "    indices_to_keep = []\n",
    "\n",
    "    # For each task_x directory, load the indices\n",
    "    for i in client_dirs:  # Task numbers 1 to 40\n",
    "\n",
    "        task_dir = os.path.join(task_x_path, f'task_{i}')\n",
    "\n",
    "        if os.path.isdir(task_dir):\n",
    "            train_indices, test_indices = load_indices(task_dir)\n",
    "            indices_to_keep.extend(train_indices)\n",
    "            test_indices_with_offset = [idx + test_offset for idx in test_indices]\n",
    "            indices_to_keep.extend(test_indices_with_offset)\n",
    "    \n",
    "    return indices_to_keep\n",
    "\n",
    "def process_images(data_dir, indices_to_keep):\n",
    "    \"\"\"\n",
    "    Process the CelebA dataset images: replace unused images with placeholders.\n",
    "    \"\"\"\n",
    "    img_dir = os.path.join(data_dir, \"celeba/img_align_celeba\")\n",
    "    assert os.path.isdir(img_dir), \"Image directory not found!\"\n",
    "\n",
    "    all_images = sorted(os.listdir(img_dir))\n",
    "\n",
    "    stored = []\n",
    "    \n",
    "    for i, img_file in enumerate(all_images):\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        if i not in indices_to_keep:\n",
    "            # Replace the image with a placeholder if it's not in the indices_to_keep list\n",
    "            create_placeholder_image(img_path)\n",
    "            # print(f\"Replaced {img_file} with a placeholder.\")\n",
    "            pass\n",
    "        else:\n",
    "            # print(f\"Kept {img_file}.\")\n",
    "            stored.append((i, img_file))\n",
    "    return stored\n",
    "\n",
    "def load_celeba_subset(data_dir, indices_to_keep):\n",
    "    \"\"\"\n",
    "    Load CelebA dataset but only access real images for indices_to_keep.\n",
    "    \"\"\"\n",
    "    # Replace unused images with placeholders to save memory\n",
    "    # process_images(data_dir, indices_to_keep)\n",
    "\n",
    "    # Load the CelebA dataset as usual\n",
    "    from torchvision.datasets import CelebA\n",
    "    from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    celeba_dataset = CelebA(\n",
    "        root=data_dir,\n",
    "        split=\"train\",  # You can adjust this based on the split you need\n",
    "        download=False,\n",
    "        transform=transform,\n",
    "        target_transform=lambda x: transform_target(x, required_labels = [31, 20, 15, 35])\n",
    "    )\n",
    "\n",
    "    # Subset the dataset to only include the indices_to_keep\n",
    "    from torch.utils.data import Subset\n",
    "    celeba_subset = Subset(celeba_dataset, indices_to_keep)\n",
    "    \n",
    "    return celeba_subset\n",
    "\n",
    "# Main function to process and load the dataset\n",
    "def main():\n",
    "    data_dir = \"/home/ubuntu/fedatk_unl_tj/data/celeba/raw_data\"  # Update with the correct path to CelebA raw data\n",
    "    task_x_path = \"/home/ubuntu/fedatk_unl_tj/data/celeba/all_data/train\"  # Path to the task_x directories\n",
    "    client_dirs = range(50)\n",
    "    # Generate the list of indices to keep\n",
    "    indices_to_keep = generate_indices_to_keep(task_x_path, client_dirs)\n",
    "\n",
    "\n",
    "    # run image replacement \n",
    "    # stored = process_images(data_dir, indices_to_keep)\n",
    "\n",
    "    # Load the CelebA dataset with the subset of indices\n",
    "    # celeba_subset = load_celeba_subset(data_dir, indices_to_keep)\n",
    "    \n",
    "    # print(f\"Loaded {len(celeba_subset)} images from CelebA dataset.\")\n",
    "    return indices_to_keep\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63873"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itk = main()\n",
    "len(itk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated elements: [182667, 182685, 182693, 182702, 182723, 182727]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "# Count occurrences\n",
    "element_counts = Counter(itk)\n",
    "\n",
    "# Find repeated elements\n",
    "repeated_elements = [item for item, count in element_counts.items() if count > 1]\n",
    "repeated_elements.sort()\n",
    "print(f\"Repeated elements: {repeated_elements}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "itk.sort()\n",
    "filtered_itk = [x for x in itk if x > 182637]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total indices from chosen pkl files: 51079\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Base path to the task directories\n",
    "base_path = '/home/ubuntu/fedatk_unl_tj/data/celeba/all_data/train'\n",
    "\n",
    "# Initialize a list to store all indices\n",
    "all_test_indices = []\n",
    "flag = []\n",
    "\n",
    "# Loop through tasks 0 to 50\n",
    "for task_num in range(50):  # 0 to 50 inclusive\n",
    "    task_path = os.path.join(base_path, f'task_{task_num}')\n",
    "    test_pkl_path = os.path.join(task_path, 'train.pkl')\n",
    "\n",
    "    # Check if the test.pkl file exists\n",
    "    if os.path.isfile(test_pkl_path):\n",
    "        # Load the indices from the test.pkl file\n",
    "        with open(test_pkl_path, 'rb') as f:\n",
    "            indices = pickle.load(f)\n",
    "            all_test_indices.extend(indices)  # Add the indices to the list\n",
    "            for element in repeated_elements:\n",
    "                if element in indices:\n",
    "                    flag.append((task_num, element))\n",
    "\n",
    "# Print the total number of indices gathered\n",
    "print(f\"Total indices from chosen pkl files: {len(all_test_indices)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 182723),\n",
       " (8, 182685),\n",
       " (15, 182702),\n",
       " (20, 182667),\n",
       " (25, 182693),\n",
       " (47, 182727)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[380,\n",
       " 404,\n",
       " 601,\n",
       " 1739,\n",
       " 1797,\n",
       " 2010,\n",
       " 2091,\n",
       " 2218,\n",
       " 2231,\n",
       " 2399,\n",
       " 2768,\n",
       " 3326,\n",
       " 3333,\n",
       " 3381,\n",
       " 3740,\n",
       " 3906,\n",
       " 3935,\n",
       " 4188,\n",
       " 4606,\n",
       " 4639,\n",
       " 4645,\n",
       " 4816,\n",
       " 4935,\n",
       " 5412,\n",
       " 5538,\n",
       " 5703,\n",
       " 5819,\n",
       " 6952,\n",
       " 6974,\n",
       " 7016,\n",
       " 7100,\n",
       " 8520,\n",
       " 8599,\n",
       " 8608,\n",
       " 9375,\n",
       " 9603,\n",
       " 9659,\n",
       " 9674,\n",
       " 9913,\n",
       " 10034,\n",
       " 10103,\n",
       " 10117,\n",
       " 10272,\n",
       " 10398,\n",
       " 11089,\n",
       " 11560,\n",
       " 11795,\n",
       " 11829,\n",
       " 11904,\n",
       " 11960,\n",
       " 12064,\n",
       " 12526,\n",
       " 12933,\n",
       " 13018,\n",
       " 13416,\n",
       " 13600,\n",
       " 13625,\n",
       " 13859,\n",
       " 13931,\n",
       " 14454,\n",
       " 14585,\n",
       " 14889,\n",
       " 15998,\n",
       " 16161,\n",
       " 16336,\n",
       " 16408,\n",
       " 16450,\n",
       " 16474,\n",
       " 16515,\n",
       " 16538,\n",
       " 16541,\n",
       " 16608,\n",
       " 16651,\n",
       " 16688,\n",
       " 17154,\n",
       " 17308,\n",
       " 17327,\n",
       " 17671,\n",
       " 18293,\n",
       " 18581,\n",
       " 18707,\n",
       " 18745,\n",
       " 18858,\n",
       " 19091,\n",
       " 19267,\n",
       " 19606,\n",
       " 19782,\n",
       " 19838,\n",
       " 19911,\n",
       " 20001,\n",
       " 20368,\n",
       " 20542,\n",
       " 20569,\n",
       " 20686,\n",
       " 20895,\n",
       " 21214,\n",
       " 21357,\n",
       " 21380,\n",
       " 21638,\n",
       " 21666,\n",
       " 21877,\n",
       " 22069,\n",
       " 22228,\n",
       " 22316,\n",
       " 22514,\n",
       " 22543,\n",
       " 22955,\n",
       " 23231,\n",
       " 23315,\n",
       " 23489,\n",
       " 24278,\n",
       " 24457,\n",
       " 24936,\n",
       " 25217,\n",
       " 25222,\n",
       " 25288,\n",
       " 25545,\n",
       " 25720,\n",
       " 25964,\n",
       " 26061,\n",
       " 26122,\n",
       " 26126,\n",
       " 26260,\n",
       " 26333,\n",
       " 26415,\n",
       " 26760,\n",
       " 26909,\n",
       " 27114,\n",
       " 27558,\n",
       " 27666,\n",
       " 28228,\n",
       " 28278,\n",
       " 28471,\n",
       " 28502,\n",
       " 28616,\n",
       " 28728,\n",
       " 29140,\n",
       " 29381,\n",
       " 29384,\n",
       " 29405,\n",
       " 29522,\n",
       " 29620,\n",
       " 29622,\n",
       " 29744,\n",
       " 29764,\n",
       " 30046,\n",
       " 31147,\n",
       " 31313,\n",
       " 31850,\n",
       " 31885,\n",
       " 32413,\n",
       " 32840,\n",
       " 33096,\n",
       " 33805,\n",
       " 34083,\n",
       " 34939,\n",
       " 34988,\n",
       " 35051,\n",
       " 35094,\n",
       " 35456,\n",
       " 35497,\n",
       " 35748,\n",
       " 35850,\n",
       " 36548,\n",
       " 36832,\n",
       " 36911,\n",
       " 37004,\n",
       " 37175,\n",
       " 37177,\n",
       " 37527,\n",
       " 37647,\n",
       " 37869,\n",
       " 38298,\n",
       " 38340,\n",
       " 38465,\n",
       " 39470,\n",
       " 40190,\n",
       " 40414,\n",
       " 40582,\n",
       " 40660,\n",
       " 40797,\n",
       " 41130,\n",
       " 41645,\n",
       " 41774,\n",
       " 42136,\n",
       " 42154,\n",
       " 42447,\n",
       " 42983,\n",
       " 43121,\n",
       " 43429,\n",
       " 43487,\n",
       " 45093,\n",
       " 45870,\n",
       " 47344,\n",
       " 47539,\n",
       " 47736,\n",
       " 47775,\n",
       " 47778,\n",
       " 47959,\n",
       " 47993,\n",
       " 48037,\n",
       " 48456,\n",
       " 48513,\n",
       " 48534,\n",
       " 48608,\n",
       " 48926,\n",
       " 49379,\n",
       " 49480,\n",
       " 49834,\n",
       " 50323,\n",
       " 50750,\n",
       " 50942,\n",
       " 51044,\n",
       " 51352,\n",
       " 51671,\n",
       " 52013,\n",
       " 52181,\n",
       " 52876,\n",
       " 52985,\n",
       " 53123,\n",
       " 53521,\n",
       " 53564,\n",
       " 53616,\n",
       " 53675,\n",
       " 54111,\n",
       " 54148,\n",
       " 54391,\n",
       " 54563,\n",
       " 55182,\n",
       " 55404,\n",
       " 55526,\n",
       " 56374,\n",
       " 56617,\n",
       " 56718,\n",
       " 57175,\n",
       " 57904,\n",
       " 58507,\n",
       " 58560,\n",
       " 58853,\n",
       " 58915,\n",
       " 58979,\n",
       " 59161,\n",
       " 59177,\n",
       " 59747,\n",
       " 59953,\n",
       " 59975,\n",
       " 60070,\n",
       " 60213,\n",
       " 60445,\n",
       " 60647,\n",
       " 61213,\n",
       " 61338,\n",
       " 61368,\n",
       " 61931,\n",
       " 62013,\n",
       " 62179,\n",
       " 62465,\n",
       " 62667,\n",
       " 62793,\n",
       " 62817,\n",
       " 63014,\n",
       " 63421,\n",
       " 63726,\n",
       " 63744,\n",
       " 63808,\n",
       " 64091,\n",
       " 64105,\n",
       " 64203,\n",
       " 64745,\n",
       " 64749,\n",
       " 64972,\n",
       " 64980,\n",
       " 65138,\n",
       " 65279,\n",
       " 66122,\n",
       " 66517,\n",
       " 66623,\n",
       " 66827,\n",
       " 67046,\n",
       " 67302,\n",
       " 68108,\n",
       " 68601,\n",
       " 68627,\n",
       " 69445,\n",
       " 69471,\n",
       " 70036,\n",
       " 70083,\n",
       " 70150,\n",
       " 70457,\n",
       " 70636,\n",
       " 70826,\n",
       " 70839,\n",
       " 70984,\n",
       " 71173,\n",
       " 71246,\n",
       " 71261,\n",
       " 71316,\n",
       " 72058,\n",
       " 72190,\n",
       " 72291,\n",
       " 72456,\n",
       " 72684,\n",
       " 72952,\n",
       " 73381,\n",
       " 73509,\n",
       " 73902,\n",
       " 74662,\n",
       " 74765,\n",
       " 74792,\n",
       " 74887,\n",
       " 74899,\n",
       " 75168,\n",
       " 75390,\n",
       " 75454,\n",
       " 75610,\n",
       " 76222,\n",
       " 76524,\n",
       " 76839,\n",
       " 76846,\n",
       " 76912,\n",
       " 77163,\n",
       " 77338,\n",
       " 77359,\n",
       " 77639,\n",
       " 78113,\n",
       " 78491,\n",
       " 78496,\n",
       " 78891,\n",
       " 78983,\n",
       " 79164,\n",
       " 79706,\n",
       " 79772,\n",
       " 80011,\n",
       " 80047,\n",
       " 80131,\n",
       " 80229,\n",
       " 80438,\n",
       " 80833,\n",
       " 81230,\n",
       " 81234,\n",
       " 81726,\n",
       " 82179,\n",
       " 82301,\n",
       " 82539,\n",
       " 82659,\n",
       " 82693,\n",
       " 83555,\n",
       " 83748,\n",
       " 83847,\n",
       " 83963,\n",
       " 84534,\n",
       " 84652,\n",
       " 85252,\n",
       " 85466,\n",
       " 85620,\n",
       " 85665,\n",
       " 85773,\n",
       " 86057,\n",
       " 86100,\n",
       " 86367,\n",
       " 86449,\n",
       " 86653,\n",
       " 86656,\n",
       " 86810,\n",
       " 87403,\n",
       " 87597,\n",
       " 88091,\n",
       " 88509,\n",
       " 88845,\n",
       " 89006,\n",
       " 89565,\n",
       " 89779,\n",
       " 89895,\n",
       " 90016,\n",
       " 90072,\n",
       " 90172,\n",
       " 90300,\n",
       " 90390,\n",
       " 90573,\n",
       " 90820,\n",
       " 91479,\n",
       " 91656,\n",
       " 91748,\n",
       " 91876,\n",
       " 92049,\n",
       " 92366,\n",
       " 92460,\n",
       " 92702,\n",
       " 92817,\n",
       " 93382,\n",
       " 93498,\n",
       " 94223,\n",
       " 94855,\n",
       " 94919,\n",
       " 95052,\n",
       " 95082,\n",
       " 95186,\n",
       " 95541,\n",
       " 95800,\n",
       " 95978,\n",
       " 96034,\n",
       " 96060,\n",
       " 96186,\n",
       " 96782,\n",
       " 96910,\n",
       " 97158,\n",
       " 97791,\n",
       " 97947,\n",
       " 98045,\n",
       " 98091,\n",
       " 98364,\n",
       " 98383,\n",
       " 98490,\n",
       " 98617,\n",
       " 99014,\n",
       " 99083,\n",
       " 99183,\n",
       " 99364,\n",
       " 99435,\n",
       " 99597,\n",
       " 100189,\n",
       " 100358,\n",
       " 100575,\n",
       " 100742,\n",
       " 101341,\n",
       " 101355,\n",
       " 101424,\n",
       " 101455,\n",
       " 101457,\n",
       " 101465,\n",
       " 101688,\n",
       " 102496,\n",
       " 102563,\n",
       " 102606,\n",
       " 102644,\n",
       " 102833,\n",
       " 103029,\n",
       " 103030,\n",
       " 103084,\n",
       " 103136,\n",
       " 103858,\n",
       " 104208,\n",
       " 104285,\n",
       " 104380,\n",
       " 104659,\n",
       " 104796,\n",
       " 105032,\n",
       " 105183,\n",
       " 105237,\n",
       " 105278,\n",
       " 105419,\n",
       " 105422,\n",
       " 105540,\n",
       " 105819,\n",
       " 105853,\n",
       " 105900,\n",
       " 105999,\n",
       " 106024,\n",
       " 106210,\n",
       " 106238,\n",
       " 106261,\n",
       " 106649,\n",
       " 106734,\n",
       " 106862,\n",
       " 106865,\n",
       " 106941,\n",
       " 106967,\n",
       " 107048,\n",
       " 107055,\n",
       " 107144,\n",
       " 107261,\n",
       " 107759,\n",
       " 107948,\n",
       " 108227,\n",
       " 108376,\n",
       " 108547,\n",
       " 108706,\n",
       " 109566,\n",
       " 110106,\n",
       " 110194,\n",
       " 110268,\n",
       " 110809,\n",
       " 111941,\n",
       " 112164,\n",
       " 112588,\n",
       " 112610,\n",
       " 112698,\n",
       " 112733,\n",
       " 112937,\n",
       " 113311,\n",
       " 113386,\n",
       " 113532,\n",
       " 113924,\n",
       " 114021,\n",
       " 114220,\n",
       " 114361,\n",
       " 114467,\n",
       " 115469,\n",
       " 115572,\n",
       " 116179,\n",
       " 116310,\n",
       " 116816,\n",
       " 116982,\n",
       " 117034,\n",
       " 117650,\n",
       " 118234,\n",
       " 118260,\n",
       " 118333,\n",
       " 118492,\n",
       " 119709,\n",
       " 119777,\n",
       " 119864,\n",
       " 119879,\n",
       " 120268,\n",
       " 120280,\n",
       " 120310,\n",
       " 120460,\n",
       " 120988,\n",
       " 121367,\n",
       " 121937,\n",
       " 121996,\n",
       " 122081,\n",
       " 123012,\n",
       " 123819,\n",
       " 124145,\n",
       " 124207,\n",
       " 124419,\n",
       " 124543,\n",
       " 124863,\n",
       " 125123,\n",
       " 125242,\n",
       " 125433,\n",
       " 125577,\n",
       " 126127,\n",
       " 126389,\n",
       " 126507,\n",
       " 127025,\n",
       " 127376,\n",
       " 127592,\n",
       " 127762,\n",
       " 128225,\n",
       " 128615,\n",
       " 128845,\n",
       " 128927,\n",
       " 129050,\n",
       " 129110,\n",
       " 129188,\n",
       " 129818,\n",
       " 129895,\n",
       " 130322,\n",
       " 130699,\n",
       " 130845,\n",
       " 131536,\n",
       " 131676,\n",
       " 131848,\n",
       " 132094,\n",
       " 132763,\n",
       " 133270,\n",
       " 133794,\n",
       " 133826,\n",
       " 134399,\n",
       " 134594,\n",
       " 134703,\n",
       " 134759,\n",
       " 135105,\n",
       " 135350,\n",
       " 135822,\n",
       " 135882,\n",
       " 135974,\n",
       " 136254,\n",
       " 136522,\n",
       " 136596,\n",
       " 136664,\n",
       " 136705,\n",
       " 137824,\n",
       " 137846,\n",
       " 138025,\n",
       " 138028,\n",
       " 138818,\n",
       " 139046,\n",
       " 139283,\n",
       " 139599,\n",
       " 139615,\n",
       " 139876,\n",
       " 140130,\n",
       " 140428,\n",
       " 140456,\n",
       " 140590,\n",
       " 140993,\n",
       " 141138,\n",
       " 141156,\n",
       " 142071,\n",
       " 142083,\n",
       " 142143,\n",
       " 142481,\n",
       " 143009,\n",
       " 143317,\n",
       " 143639,\n",
       " 143853,\n",
       " 144322,\n",
       " 144673,\n",
       " 145603,\n",
       " 145671,\n",
       " 145931,\n",
       " 146289,\n",
       " 146331,\n",
       " 146484,\n",
       " 146633,\n",
       " 147020,\n",
       " 147467,\n",
       " 147483,\n",
       " 147576,\n",
       " 147792,\n",
       " 147837,\n",
       " 147994,\n",
       " 148463,\n",
       " 148550,\n",
       " 148988,\n",
       " 149219,\n",
       " 149243,\n",
       " 149264,\n",
       " 149679,\n",
       " 149824,\n",
       " 149854,\n",
       " 150130,\n",
       " 150191,\n",
       " 150211,\n",
       " 150437,\n",
       " 150459,\n",
       " 150649,\n",
       " 151381,\n",
       " 151497,\n",
       " 151594,\n",
       " 151721,\n",
       " 151902,\n",
       " 152015,\n",
       " 152261,\n",
       " 152293,\n",
       " 152345,\n",
       " 152552,\n",
       " 152720,\n",
       " 153618,\n",
       " 153988,\n",
       " 154084,\n",
       " 154286,\n",
       " 154375,\n",
       " 154458,\n",
       " 154496,\n",
       " 154818,\n",
       " 155026,\n",
       " 155228,\n",
       " 155609,\n",
       " 155623,\n",
       " 156288,\n",
       " 156332,\n",
       " 156395,\n",
       " 156437,\n",
       " 156491,\n",
       " 156627,\n",
       " 157020,\n",
       " 157081,\n",
       " 157129,\n",
       " 157148,\n",
       " 157569,\n",
       " 157823,\n",
       " 158367,\n",
       " 158790,\n",
       " 158933,\n",
       " 159155,\n",
       " 159280,\n",
       " 159357,\n",
       " 159765,\n",
       " 159787,\n",
       " 160355,\n",
       " 160521,\n",
       " 160717,\n",
       " 160815,\n",
       " 161094,\n",
       " 161473,\n",
       " 161523,\n",
       " 161590,\n",
       " 162171,\n",
       " 162203,\n",
       " 162642,\n",
       " 162685,\n",
       " 163544,\n",
       " 163866,\n",
       " 164629,\n",
       " 164673,\n",
       " 165270,\n",
       " 165331,\n",
       " 165365,\n",
       " 165758,\n",
       " 165847,\n",
       " 166018,\n",
       " 166035,\n",
       " 166414,\n",
       " 166448,\n",
       " 166682,\n",
       " 166724,\n",
       " 166881,\n",
       " 167413,\n",
       " 167561,\n",
       " 168691,\n",
       " 168772,\n",
       " 168940,\n",
       " 169193,\n",
       " 169223,\n",
       " 169334,\n",
       " 169528,\n",
       " 169700,\n",
       " 170323,\n",
       " 170846,\n",
       " 170911,\n",
       " 171161,\n",
       " 171182,\n",
       " 171331,\n",
       " 171564,\n",
       " 171638,\n",
       " 171860,\n",
       " 171949,\n",
       " 171957,\n",
       " 172049,\n",
       " 172117,\n",
       " 172303,\n",
       " 172870,\n",
       " 173009,\n",
       " 173179,\n",
       " 173255,\n",
       " 173336,\n",
       " 173526,\n",
       " 173546,\n",
       " 173710,\n",
       " 174048,\n",
       " 174543,\n",
       " 174567,\n",
       " 174969,\n",
       " 175018,\n",
       " 175542,\n",
       " 175639,\n",
       " 176109,\n",
       " 176171,\n",
       " 176646,\n",
       " 177623,\n",
       " 177729,\n",
       " 178132,\n",
       " 178145,\n",
       " 178404,\n",
       " 178459,\n",
       " 178824,\n",
       " 178861,\n",
       " 179567,\n",
       " 179601,\n",
       " 179690,\n",
       " 179753,\n",
       " 179780,\n",
       " 179795,\n",
       " 179984,\n",
       " 180025,\n",
       " 180263,\n",
       " 180606,\n",
       " 180960,\n",
       " 181090,\n",
       " 181183,\n",
       " 181377,\n",
       " 181562,\n",
       " 181566,\n",
       " 181669,\n",
       " 182118,\n",
       " 182453,\n",
       " 182473,\n",
       " 182625,\n",
       " 182723]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_path = os.path.join(base_path, f'task_{5}')\n",
    "test_pkl_path = os.path.join(task_path, 'train.pkl')\n",
    "\n",
    "with open(test_pkl_path,'rb') as f:\n",
    "    aa = pickle.load(f)\n",
    "\n",
    "aa.sort()\n",
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from torchvision.datasets import CelebA\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import gc\n",
    "import os\n",
    "\n",
    "def transform_target(x, required_labels):\n",
    "    return [x[label] for label in required_labels]\n",
    "\n",
    "def get_celeba():\n",
    "    celeba_path = os.path.join(\"data\", \"celeba\", \"raw_data\")\n",
    "    assert os.path.isdir(celeba_path), \"Download celeba dataset!\"\n",
    "    \n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(\n",
    "            (0.4914, 0.4822, 0.4465),\n",
    "            (0.2023, 0.1994, 0.2010)\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "    # Load train indices\n",
    "    train_idx = np.load('data/celeba/train_idx.npy', allow_pickle=True)\n",
    "    print(\"loading train set\")\n",
    "    # Process train set\n",
    "    celeba_train = CelebA(\n",
    "        root=celeba_path,\n",
    "        split='train',\n",
    "        download=False,\n",
    "        transform=transform,\n",
    "        target_transform=lambda x: transform_target(x, required_labels=[31, 20, 15, 35])  # Smiling, Male, Eyeglasses, Wearing Hat\n",
    "    )\n",
    "    print(\"got train set processing. ..\")\n",
    "    # celeba_train = Subset(celeba_train, train_idx)\n",
    "\n",
    "    celeba_data_X_train = []\n",
    "    celeba_data_y_train = []\n",
    "    for data in celeba_train:\n",
    "        celeba_data_X_train.append(data[0].numpy())\n",
    "        y_lab = int(sum(bit.item() * (2 ** idx) for idx, bit in enumerate(reversed((data[1])))))\n",
    "        celeba_data_y_train.append(y_lab)\n",
    "    print(\"made new list ...\")\n",
    "\n",
    "\n",
    "    # Release memory for train set\n",
    "    del celeba_train\n",
    "    gc.collect()\n",
    "\n",
    "    # Load test indices\n",
    "    test_idx = np.load('data/celeba/test_idx.npy', allow_pickle=True)\n",
    "    print(\"loading test set\")\n",
    "    # Process test set\n",
    "    celeba_test = CelebA(\n",
    "        root=celeba_path,\n",
    "        split='test',\n",
    "        download=False,\n",
    "        transform=transform,\n",
    "        target_transform=lambda x: transform_target(x, required_labels=[31, 20, 15, 35])\n",
    "    )\n",
    "    # celeba_test = Subset(celeba_test, test_idx)\n",
    "\n",
    "    celeba_data_X_test = []\n",
    "    celeba_data_y_test = []\n",
    "    for data in celeba_test:\n",
    "        celeba_data_X_test.append(data[0])\n",
    "        celeba_data_y_test.append(data[1])\n",
    "\n",
    "    # Release memory for test set\n",
    "    del celeba_test\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Combine train and test data\n",
    "    celeba_data_X = torch.stack(celeba_data_X_train + celeba_data_X_test)\n",
    "    celeba_data_y = torch.Tensor(celeba_data_y_train + celeba_data_y_test)\n",
    "\n",
    "    return celeba_data_X, celeba_data_y\n",
    "\n",
    "def get_celeba_new(batch_size=2048):\n",
    "    celeba_path = os.path.join(\"data\", \"celeba\", \"raw_data\")\n",
    "    assert os.path.isdir(celeba_path), \"Download CelebA dataset!\"\n",
    "    \n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(\n",
    "            (0.4914, 0.4822, 0.4465),\n",
    "            (0.2023, 0.1994, 0.2010)\n",
    "        )\n",
    "    ])\n",
    "    \n",
    "\n",
    "    # Load train and test indices\n",
    "    train_idx = np.load('data/celeba/train_idx.npy', allow_pickle=True)\n",
    "    test_idx = np.load('data/celeba/test_idx.npy', allow_pickle=True)\n",
    "    \n",
    "    # Load train set (Lazy loading via DataLoader)\n",
    "    print(\"Loading train set...\")\n",
    "    celeba_train = datasets.CelebA(\n",
    "        root=celeba_path,\n",
    "        split='train',\n",
    "        download=False,\n",
    "        transform=transform,\n",
    "        target_transform=lambda x: transform_target(x, required_labels=[31, 20, 15, 35])  # Smiling, Male, Eyeglasses, Wearing Hat\n",
    "    )\n",
    "\n",
    "    # Filter train set with the desired indices\n",
    "    celeba_train = torch.utils.data.Subset(celeba_train, train_idx)\n",
    "    \n",
    "    # Create DataLoader for batch loading\n",
    "    train_loader = DataLoader(celeba_train, batch_size=batch_size, shuffle=False)\n",
    "    celeba_data_X_train = []\n",
    "    celeba_data_y_train = []\n",
    "\n",
    "    # Process train set in batches\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        # Perform your processing here (training or analysis)\n",
    "        # For example, data is a batch of images, targets are the labels\n",
    "        print(f\"Processed batch {batch_idx + 1}/{len(train_loader)}\")\n",
    "        celeba_data_X_train.append(data.numpy())\n",
    "        celeba_data_y_train.append(batch_labels(targets))\n",
    "\n",
    "        # Clear cache after processing a batch to free memory\n",
    "        del data, targets\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Load test set (Lazy loading via DataLoader)\n",
    "    print(\"Loading test set...\")\n",
    "    celeba_test = datasets.CelebA(\n",
    "        root=celeba_path,\n",
    "        split='test',\n",
    "        download=False,\n",
    "        transform=transform,\n",
    "        target_transform=lambda x: transform_target(x, required_labels=[31, 20, 15, 35])\n",
    "    )\n",
    "\n",
    "    # Filter test set with the desired indices\n",
    "    celeba_test = torch.utils.data.Subset(celeba_test, test_idx)\n",
    "    \n",
    "    # Create DataLoader for batch loading\n",
    "    test_loader = DataLoader(celeba_test, batch_size=batch_size, shuffle=False)\n",
    "    celeba_data_X_test = []\n",
    "    celeba_data_y_test = []\n",
    "\n",
    "    # Process test set in batches\n",
    "    for batch_idx, (data, targets) in enumerate(test_loader):\n",
    "        # Perform your processing here (evaluation or analysis)\n",
    "        print(f\"Processed test batch {batch_idx + 1}/{len(test_loader)}\")\n",
    "        celeba_data_X_test.append(data.numpy())\n",
    "        celeba_data_y_test.append(batch_labels(targets))\n",
    "        # Clear cache after processing a batch to free memory\n",
    "        del data, targets\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    print(\"Data loading complete.\")\n",
    "    x = np.concatenate(celeba_data_X_train + celeba_data_X_test, axis = 0)\n",
    "    y = np.concatenate(celeba_data_y_train + celeba_data_y_test, axis = 0)\n",
    "\n",
    "    x_torch = torch.from_numpy(x)\n",
    "    y_torch = torch.from_numpy(y)\n",
    "    return x, y\n",
    "\n",
    "def batch_labels(targets):\n",
    "    stacked_labels = torch.stack(targets) \n",
    "    transposed_labels = stacked_labels.T\n",
    "    processed_labels = transposed_labels.tolist()\n",
    "    \n",
    "    return [int(\"\".join(map(str, sublist)), 2) for sublist in processed_labels]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train set...\n",
      "Processed batch 1/80\n",
      "Processed batch 2/80\n",
      "Processed batch 3/80\n",
      "Processed batch 4/80\n",
      "Processed batch 5/80\n",
      "Processed batch 6/80\n",
      "Processed batch 7/80\n",
      "Processed batch 8/80\n",
      "Processed batch 9/80\n",
      "Processed batch 10/80\n",
      "Processed batch 11/80\n",
      "Processed batch 12/80\n",
      "Processed batch 13/80\n",
      "Processed batch 14/80\n",
      "Processed batch 15/80\n",
      "Processed batch 16/80\n",
      "Processed batch 17/80\n",
      "Processed batch 18/80\n",
      "Processed batch 19/80\n",
      "Processed batch 20/80\n",
      "Processed batch 21/80\n",
      "Processed batch 22/80\n",
      "Processed batch 23/80\n",
      "Processed batch 24/80\n",
      "Processed batch 25/80\n",
      "Processed batch 26/80\n",
      "Processed batch 27/80\n",
      "Processed batch 28/80\n",
      "Processed batch 29/80\n",
      "Processed batch 30/80\n",
      "Processed batch 31/80\n",
      "Processed batch 32/80\n",
      "Processed batch 33/80\n",
      "Processed batch 34/80\n",
      "Processed batch 35/80\n",
      "Processed batch 36/80\n",
      "Processed batch 37/80\n",
      "Processed batch 38/80\n",
      "Processed batch 39/80\n",
      "Processed batch 40/80\n",
      "Processed batch 41/80\n",
      "Processed batch 42/80\n",
      "Processed batch 43/80\n",
      "Processed batch 44/80\n",
      "Processed batch 45/80\n",
      "Processed batch 46/80\n",
      "Processed batch 47/80\n",
      "Processed batch 48/80\n",
      "Processed batch 49/80\n",
      "Processed batch 50/80\n",
      "Processed batch 51/80\n",
      "Processed batch 52/80\n",
      "Processed batch 53/80\n",
      "Processed batch 54/80\n",
      "Processed batch 55/80\n",
      "Processed batch 56/80\n",
      "Processed batch 57/80\n",
      "Processed batch 58/80\n",
      "Processed batch 59/80\n",
      "Processed batch 60/80\n",
      "Processed batch 61/80\n",
      "Processed batch 62/80\n",
      "Processed batch 63/80\n",
      "Processed batch 64/80\n",
      "Processed batch 65/80\n",
      "Processed batch 66/80\n",
      "Processed batch 67/80\n",
      "Processed batch 68/80\n",
      "Processed batch 69/80\n",
      "Processed batch 70/80\n",
      "Processed batch 71/80\n",
      "Processed batch 72/80\n",
      "Processed batch 73/80\n",
      "Processed batch 74/80\n",
      "Processed batch 75/80\n",
      "Processed batch 76/80\n",
      "Processed batch 77/80\n",
      "Processed batch 78/80\n",
      "Processed batch 79/80\n",
      "Processed batch 80/80\n",
      "Loading test set...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_89532/1678915717.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_celeba_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_89532/614810763.py\u001b[0m in \u001b[0;36mget_celeba_new\u001b[0;34m(batch_size)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;31m# Process test set in batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Perform your processing here (evaluation or analysis)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Processed test batch {batch_idx + 1}/{len(test_loader)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
      "\u001b[0;32m~/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/torchvision/datasets/celeba.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"img_align_celeba\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/FedEM_env/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3254\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3256\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3258\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x,y=get_celeba_new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(\n",
    "            (0.4914, 0.4822, 0.4465),\n",
    "            (0.2023, 0.1994, 0.2010)\n",
    "        )\n",
    "    ])\n",
    "train_data = CelebA(root='data/celeba/raw_data/', download=False, split='train', transform=transform, target_transform=lambda x: transform_target(x, required_labels = [31, 20, 15, 35])) # Smiling, Male, Eyeglasses, Wearing Hat\n",
    "test_data= CelebA(root='data/celeba/raw_data/', download=False, split='test', transform=transform, target_transform=lambda x: transform_target(x, required_labels = [31, 20, 15, 35])) # Smiling, Male, Eyeglasses, Wearing Hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/318\n",
      "Processed batch 2/318\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=512, shuffle=False)\n",
    "celeba_data_X_train = []\n",
    "celeba_data_y_train = []\n",
    "\n",
    "for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "    # Perform your processing here (training or analysis)\n",
    "    # For example, data is a batch of images, targets are the labels\n",
    "    print(f\"Processed batch {batch_idx + 1}/{len(train_loader)}\")\n",
    "    celeba_data_X_train.append(data.numpy())\n",
    "    celeba_data_y_train.append(batch_labels(targets))\n",
    "    # Clear cache after processing a batch to free memory\n",
    "    del data, targets\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    if batch_idx == 1:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate(celeba_data_X_train + celeba_data_X_train, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.concatenate(celeba_data_y_train, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 8, 4, 0, 0])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048, 3, 55, 45)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FedEM_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
